{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from itertools import chain\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest ,chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMporting the training document\n",
    "traindoc = open('C:/Users/mrutula1995/Documents/ada/data/training_docs.txt',mode = \"r\",encoding=\"utf-8-sig\")\n",
    "train = traindoc.readlines()\n",
    "traindoc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the test document\n",
    "testdoc = open('C:/Users/mrutula1995/Documents/ada/data/testing_docs.txt',mode = \"r\",encoding=\"utf-8-sig\")\n",
    "test = testdoc.readlines()\n",
    "testdoc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training labels\n",
    "trainlabel = open('C:/Users/mrutula1995/Documents/ada/data/training_labels_final.txt',mode = 'r',encoding = \"utf-8-sig\")\n",
    "trainLB = trainlabel.readlines()\n",
    "trainlabel.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing space from the labels\n",
    "trainLB = [t.strip() for t in trainLB]\n",
    "trainLB = [t.split()[1] for t in trainLB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425780"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the ID of the Train document \n",
    "i = 0\n",
    "col_id = []         \n",
    "while i < len(train):\n",
    "    col_id.append(train[i])    \n",
    "    i += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the ID of the Test document \n",
    "i = 0\n",
    "test_col_id = []         \n",
    "while i < len(test):\n",
    "    test_col_id.append(test[i])    \n",
    "    i += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_id = [x.strip() for x in col_id]     #to remove the \\n in the end\n",
    "col_id = [re.sub(r'^(ID\\s)','',i) for i in col_id]  #Replacing 'ID' with empty character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col_id = [x.strip() for x in test_col_id]     #to remove the \\n in the end\n",
    "test_col_id = [re.sub(r'^(ID\\s)','',i) for i in test_col_id] #Replacing 'ID' with empty character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the txt of the Train document\n",
    "i = 1\n",
    "text = []           \n",
    "while i < len(train):\n",
    "    text.append(train[i])\n",
    "    i += 4    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the txt of Test the document\n",
    "i = 1\n",
    "text_test = []           \n",
    "while i < len(test):\n",
    "    text_test.append(test[i])\n",
    "    i += 4    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the space from the train text \n",
    "text = [t.strip() for t in text]\n",
    "#Replacing the word TEXT with empty character\n",
    "text = [re.sub(r'^(TEXT)','',t) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the space from the test text \n",
    "text_test = [t.strip() for t in text_test]\n",
    "#Replacing the word TEXT with empty character\n",
    "text_test = [re.sub(r'^(TEXT)','',t) for t in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train dataframe\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['ID'] = col_id\n",
    "trainDF['text'] = text\n",
    "trainDF['label'] = trainLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating test dataframe\n",
    "testDF = pd.DataFrame()\n",
    "testDF['ID'] = test_col_id\n",
    "testDF['text'] = text_test\n",
    "#testDF['label'] = testLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF[trainDF.text != \" '\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the train text\n",
    "trainDF[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "trainDF[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "trainDF[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "trainDF[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "trainDF[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "trainDF[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the test text\n",
    "#testDF['num_of_words'] = testDF[\"text\"].str.split().apply(len)\n",
    "testDF[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "testDF[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "testDF[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "testDF[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "testDF[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "testDF[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PorterStemmer object\n",
    "porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    #text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "\n",
    "#Preprocessing function to stem the sentence\n",
    "def clearstring(string):\n",
    "    try:\n",
    "        string = re.sub('[^A-Za-z .]+', '', string)\n",
    "        string = string.split(' ')\n",
    "        string = filter(None, string)\n",
    "        string = [y.strip() for y in string]        \n",
    "        string = [porter_stemmer.stem(word) for word in string]\n",
    "        string = ' '.join(string)\n",
    "    except:\n",
    "        print(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trainDF.shape[0]):\n",
    "    trainDF['text'].iloc[i] = clearstring(trainDF['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(testDF.shape[0]):\n",
    "    testDF['text'].iloc[i] = clearstring(testDF['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just want verbs and nouns in a sentence\n",
    "def get_clean_text(string):\n",
    "    blob = TextBlob(string).tags\n",
    "    tags = []\n",
    "    # you can add more\n",
    "    accept = ['NNP', 'NN', 'NNS', 'NNPS', 'VBZ', 'VBN', 'VB']\n",
    "    for k in blob:\n",
    "        if k[1] in accept:\n",
    "            tags.append(k[0])\n",
    "            \n",
    "    return list(OrderedDict.fromkeys(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to split by ('.') to save our memory during speech tagging process to build the tree\n",
    "for i in range(trainDF.shape[0]):\n",
    "    texts = trainDF['text'].iloc[i].split('. ')\n",
    "    tags = []\n",
    "    for t in texts: \n",
    "        tags += get_clean_text(t)\n",
    "    trainDF['text'].iloc[i] = ' '.join(list(OrderedDict.fromkeys(tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to split by ('.') to save our memory during speech tagging process to build the tree\n",
    "for i in range(testDF.shape[0]):\n",
    "    texts = testDF['text'].iloc[i].split('. ')\n",
    "    tags = []\n",
    "    for t in texts:\n",
    "        tags += get_clean_text(t)\n",
    "    testDF['text'].iloc[i] = ' '.join(list(OrderedDict.fromkeys(tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'act drug dure home number oxblood is bag believ woman been face court morning varley time ha biproduct amphetamin manufactur process wherebi powder crystal produc methamphetamin phospher iodin colour'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'govern is queensland respons explain deni transfer alic spring prison tommi neale neal court challeng decision convict murder mount isa num request territori law serv life sentenc parole lawyer have been abl appli parol year kim aborigin aid ha stonewal effort find applic be wa reject attorney gener peter toyn correspond repres correct minist toni mcgradi refus becaus behaviour sustain famili grounds hi offic statement stands'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainDF['label'].values.copy()\n",
    "\n",
    "# get unique label\n",
    "unique_labels = np.unique(labels)\n",
    "# change into int\n",
    "labels = LabelEncoder().fit_transform(labels)\n",
    "texts = trainDF['text'].values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "     ('text', TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=10000)),\n",
    "     ('text', TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.8,max_features=10000))\n",
    "    \n",
    " ],df_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data = mapper.fit_transform(X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mapper = mapper.transformed_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mapper = [re.sub(r'text_','',l) for l in list_mapper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer_train =  TfidfVectorizer(analyzer='word',ngram_range=(1, 2),max_df=0.5,norm='l2',max_features=2000)\n",
    "tfidf_train = tfidf_transformer_train.fit_transform(trainDF.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = pd.DataFrame(tfidf_train.toarray(), columns=tfidf_transformer_train.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat['label'] = trainDF['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_mat.to_csv(\"H_train_data.csv\",index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer_test =  TfidfVectorizer(analyzer='word',ngram_range=(1, 2),max_df=0.5,norm='l2',max_features=2000)\n",
    "tfidf_test = tfidf_transformer_test.fit_transform(testDF.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mat = pd.DataFrame(tfidf_test.toarray(), columns=tfidf_transformer_test.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = list(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = train_mat.drop(labels = 'label',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name = list(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = pd.DataFrame()\n",
    "for t in test_mat:\n",
    "    #print(t)\n",
    "    if t in train_name:\n",
    "        test_name[t] = test_mat[t]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = [x for x in train_name if x not in test_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alloc'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in diff:\n",
    "    test_name[d] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name.to_csv(\"H_test_data.csv\",index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = pd.DataFrame(tfidf_train.todense(), index=data.index, columns=cv.get_feature_names())\n",
    "train_mat['number'] = data.number\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD Classifier is used\n",
    "magazine_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                         ('tfidf', TfidfTransformer()), \n",
    "                         ('clf', SGDClassifier(loss = 'modified_huber', \n",
    "                                               penalty = 'l2', alpha = 1e-4, \n",
    "                                               n_iter = 100, random_state = 42))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrutula1995\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7756118183099253\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(texts, labels, test_size = 0.2)\n",
    "magazine_clf.fit(x_train, y_train)\n",
    "predicted = magazine_clf.predict(x_test)\n",
    "print (np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "magazine_clf = Pipeline([('vect',TfidfVectorizer(ngram_range = (1,2), stop_words = \"english\",max_df = 0.99,sublinear_tf= True)),\n",
    "                         ('chi',SelectKBest(chi2,k=50000)),\n",
    "                          ('clf', SGDClassifier(loss = 'modified_huber', \n",
    "                                               penalty = 'l2', alpha = 1e-4, \n",
    "                                               n_iter = 100, random_state = 42))])\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrutula1995\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7532528535863592\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(texts, labels, test_size = 0.2)\n",
    "magazine_clf.fit(x_train, y_train)\n",
    "predicted = magazine_clf.predict(x_test)\n",
    "print (np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrutula1995\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\mrutula1995\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\mrutula1995\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\mrutula1995\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\mrutula1995\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(magazine_clf,texts,labels,cv=5,scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
